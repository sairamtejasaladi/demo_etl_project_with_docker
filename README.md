# ETL Pipeline with Apache Airflow and Docker

## Overview
This project implements an **ETL (Extract, Transform, Load) pipeline** using **Apache Airflow** running inside **Docker**. The pipeline automates data ingestion, transformation, and storage using Airflow DAGs.

## Features
- âœ… **Containerized Airflow Setup** using Docker
- âœ… **Automated ETL Workflow** managed by Airflow
- âœ… **Modular and Scalable** pipeline design
- âœ… **Logging and Monitoring** for troubleshooting
- âœ… **Custom Operators and Tasks** for data processing

## Project Structure
```
etl_project/
â”‚â”€â”€ dags/                  # Airflow DAGs for ETL pipeline
â”‚   â”œâ”€â”€ etl_pipeline.py    # Main ETL DAG
â”‚â”€â”€ scripts/               # Supporting scripts
â”‚   â”œâ”€â”€ extract.py         # Data extraction logic
â”‚   â”œâ”€â”€ transform.py       # Data transformation logic
â”‚   â”œâ”€â”€ load.py            # Data loading logic
â”‚â”€â”€ config/                # Configuration files
â”‚â”€â”€ logs/                  # Airflow logs
â”‚â”€â”€ plugins/               # Custom Airflow plugins
â”‚â”€â”€ docker-compose.yaml    # Docker Compose file for Airflow setup
â”‚â”€â”€ requirements.txt       # Python dependencies
â”‚â”€â”€ README.md              # Project documentation
```

## Installation & Setup

### Prerequisites
- **Docker & Docker Compose** installed
- **Python 3.8+** (if running locally without Docker)

### Steps to Set Up the Project

1. **Clone the Repository**
   ```bash
   git clone https://github.com/yourusername/etl_project.git
   cd etl_project
   ```

2. **Start Airflow with Docker**
   ```bash
   docker-compose up -d
   ```

3. **Check Airflow UI**
   - Open **http://localhost:8080** in your browser  
   - Default **credentials**:
     - Username: `airflow`
     - Password: `airflow`

4. **Trigger the ETL Pipeline**
   - Navigate to the **DAGs** tab in Airflow UI
   - Enable and trigger `etl_pipeline` DAG

## Environment Variables
Create a `.env` file in the root directory and define:
```
DATABASE_URL=postgresql://user:password@host:port/dbname
S3_BUCKET_NAME=my-etl-bucket
```

## Airflow Commands

- **Check running containers:**
  ```bash
  docker ps
  ```
- **Restart Airflow containers:**
  ```bash
  docker-compose restart
  ```
- **View Airflow logs:**
  ```bash
  docker-compose logs -f
  ```

## Future Enhancements
ðŸš€ Add database integration (PostgreSQL, BigQuery, etc.)  
ðŸš€ Implement error handling & notifications  
ðŸš€ Optimize DAG execution time  

---

This README provides all necessary details to set up and run the project. ðŸš€

